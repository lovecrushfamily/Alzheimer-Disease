Build 1 web page để run cái đống model kia, tiện thể quay lại web 1 chút với flask, steamlit

trên web đó sẽ là 1 web kiểu chuẩn đoán rất nhiều bệnh
- Thận nè
- Alzheimer nè
- 

Trên web có những cái form, có thể load được 1 người đơn lẻ hoặc 1 file csv, excel, nhưng phải có preview trước cơ. và có thể chọn được model để mình predict, coi như những con bot của các bác sĩ.

Cuối cùng thì phân tích những hiệu quả, hành vi của từng model trên các bộ dữ liệu khác nhau. với những con số khác nhau.




*Làm thêm phần hyperparameter và SMote for class imbalances


adding all the problems into your dataset and find a way to solve it. like what I am


dataleakage
multicolinearity
similarity and dissimilarity

dimensionality reduction     => PCA

Feature extraction
feature selection

Feature creation : cai nay kho, tao ra feature, feature engineering



Imbalanced class handled.

stratified sampling for train and test set



Explain skewness, std, correlation, kurtosis, variance








The only difference is that make_pipeline generates names for steps automatically.

Step names are needed e.g. if you want to use a pipeline with model selection utilities (e.g. GridSearchCV). With grid search you need to specify parameters for various steps of a pipeline:

pipe = Pipeline([('vec', CountVectorizer()), ('clf', LogisticRegression()])
param_grid = [{'clf__C': [1, 10, 100, 1000]}
gs = GridSearchCV(pipe, param_grid)
gs.fit(X, y)

compare it with make_pipeline:

pipe = make_pipeline(CountVectorizer(), LogisticRegression())     
param_grid = [{'logisticregression__C': [1, 10, 100, 1000]}
gs = GridSearchCV(pipe, param_grid)
gs.fit(X, y)

So, with Pipeline:

    names are explicit, you don't have to figure them out if you need them;
    name doesn't change if you change estimator/transformer used in a step, e.g. if you replace LogisticRegression() with LinearSVC() you can still use clf__C.

make_pipeline:

    shorter and arguably more readable notation;
    names are auto-generated using a straightforward rule (lowercase name of an estimator).

When to use them is up to you :) I prefer make_pipeline for quick experiments and Pipeline for more stable code; a rule of thumb: IPython Notebook -> make_pipeline; Python module in a larger project -> Pipeline. But it is certainly not a big deal to use make_pipeline in a module or Pipeline in a short script or a notebook.







Create traning pipeline and inferences pipeline


training pipeline => fit method

inferences pipeline => predict method










-------------------------------------------------------------------------------------------
What was saved in the pickle model
How to bring model in production







Using association rule mining to finding frequent pattern and using the pattern and support to create new feature